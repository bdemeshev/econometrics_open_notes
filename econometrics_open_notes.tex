% arara: xelatex
\documentclass[12pt]{article}

% \usepackage{physics} % это трэшовый пакет!

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\urlstyle{same} % шрифт из основного текста

\usepackage{verse}

\usepackage{ulem} % перечёркнутый шрифт \sout{что-то}

\usepackage{tikzducks}

\usepackage{tikz} % картинки в tikz
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes, arrows, positioning}
\usepackage{microtype} % свешивание пунктуации

\usepackage{array} % для столбцов фиксированной ширины

\usepackage{indentfirst} % отступ в первом параграфе

\usepackage{sectsty} % для центрирования названий частей
\allsectionsfont{\centering}

\usepackage{amsmath, amsfonts, amssymb, amsthm} % куча стандартных математических плюшек

\usepackage{comment}

\usepackage[top=2cm, left=1.2cm, right=1.2cm, bottom=2cm]{geometry} % размер текста на странице

\usepackage{lastpage} % чтобы узнать номер последней страницы

\usepackage{enumitem} % дополнительные плюшки для списков
%  например \begin{enumerate}[resume] позволяет продолжить нумерацию в новом списке
\usepackage{caption}

\usepackage{url} % to use \url{link to web}

\newcommand{\smallduck}{\begin{tikzpicture}[scale=0.3]
    \duck[
        cape=black,
        hat=black,
        mask=black
    ]
    \end{tikzpicture}}

\usepackage{fancyhdr} % весёлые колонтитулы
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\usepackage{tcolorbox} % рамочки!

\usepackage{todonotes} % для вставки в документ заметок о том, что осталось сделать
% \todo[inline]{Здесь надо коэффициенты исправить}
% \missingfigure{Здесь будет Последний день Помпеи}
% \listoftodos - печатает все поставленные \todo'шки


% более красивые таблицы
\usepackage{booktabs}
% заповеди из докупентации:
% 1. Не используйте вертикальные линни
% 2. Не используйте двойные линии
% 3. Единицы измерения - в шапку таблицы
% 4. Не сокращайте .1 вместо 0.1
% 5. Повторяющееся значение повторяйте, а не говорите "то же"


\setcounter{MaxMatrixCols}{20}
% by crazy default pmatrix supports only 10 cols :)


\usepackage{fontspec}
\usepackage{libertine}
\usepackage{polyglossia}

\setmainlanguage{russian}
\setotherlanguages{english}

% download "Linux Libertine" fonts:
% http://www.linuxlibertine.org/index.php?id=91&L=1
% \setmainfont{Linux Libertine O} % or Helvetica, Arial, Cambria
% why do we need \newfontfamily:
% http://tex.stackexchange.com/questions/91507/
% \newfontfamily{\cyrillicfonttt}{Linux Libertine O}

\AddEnumerateCounter{\asbuk}{\russian@alph}{щ} % для списков с русскими буквами
\setlist[enumerate, 1]{label=\asbuk*),ref=\asbuk*}

%% эконометрические сокращения
\DeclareMathOperator{\IMR}{IMR} % Inverse Mill's ratio
\DeclareMathOperator{\Cov}{\mathbb{C}ov}
\DeclareMathOperator{\Corr}{\mathbb{C}orr}
\DeclareMathOperator{\Var}{\mathbb{V}ar}
\DeclareMathOperator{\hVar}{\widehat{\Var}}
\DeclareMathOperator{\hCov}{\widehat{\Cov}}
\DeclareMathOperator{\hCorr}{\widehat{\Corr}}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\BestLin}{BestLin}

\let\P\relax
\DeclareMathOperator{\P}{\mathbb{P}}

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tr}{\trace}
\DeclareMathOperator{\rk}{\rank}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\mgf}{mgf}

\DeclareMathOperator{\Convex}{Convex}
\DeclareMathOperator{\plim}{plim}

\usepackage{mathtools}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\scalp}{\langle}{\rangle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newcommand{\cN}{\mathcal{N}}
\newcommand{\cF}{\mathcal{F}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}

\newcommand{\dBern}{\mathrm{Bern}}
\newcommand{\dPois}{\mathrm{Pois}}
\newcommand{\dBin}{\mathrm{Bin}}
\newcommand{\dMult}{\mathrm{Mult}}
\newcommand{\dGeom}{\mathrm{Geom}}
\newcommand{\dNHGeom}{\mathrm{NHGeom}}
\newcommand{\dHGeom}{\mathrm{HGeom}}
\newcommand{\dDUnif}{\mathrm{DUnif}}
\newcommand{\dFS}{\mathrm{FS}}
\newcommand{\dNBin}{\mathrm{NBin}}

\newcommand{\dTri}{\mathrm{Triangle}}
\newcommand{\dUnif}{\mathrm{Unif}}
\newcommand{\dCauchy}{\mathrm{Cauchy}}
\newcommand{\dN}{\mathcal{N}}
\newcommand{\dLN}{\mathcal{LN}}
\newcommand{\dExpo}{\mathrm{Expo}}
\newcommand{\dBeta}{\mathrm{Beta}}
\newcommand{\dGamma}{\mathrm{Gamma}}
\newcommand{\dWei}{\mathrm{Wei}}
\newcommand{\dLogistic}{\mathrm{Logistic}}
\newcommand{\dRayleigh}{\mathrm{Rayleigh}}
\newcommand{\dPareto}{\mathrm{Pareto}}


\renewcommand{\b}{\beta}
\newcommand{\hb}{\hat{\beta}}
\newcommand{\tb}{\tilde{\beta}}
\renewcommand{\u}{u}
\newcommand{\hu}{\hat{u}}
\newcommand{\hs}{\hat{\sigma}}
\newcommand{\hy}{\hat{y}}

\newcommand{\RSS}{RSS}
\newcommand{\ESS}{ESS}
\newcommand{\TSS}{TSS}


\newcommand{\alt}{\text{alt}}
\newcommand{\adj}{\text{adj}}
\newcommand{\best}{\text{best}}
\newcommand{\ols}{\text{ols}}


\newcommand{\gaussmarkov}{
\begin{enumerate}[label=\arabic*.]
    \item Модель линейна по параметрам: $y = X\b + u$;
    \item Матрица $X$ размера $[n \times k]$ имеет полный ранг $k$.
    \item Условное ожидание ошибок равно нулю, $\E(\u \mid X) = 0$;
    \item Условная ковариационная матрица ошибок пропорциональна единичной, $\Var(\u \mid X) = \sigma^2 I$;
    \item Оценка $\hb$ получена методом наименьших квадратов, $\hb = (X^T X)^{-1}X^T y$;
\end{enumerate}}

\usepackage{thmtools} % тонкая настройка окружения теорем


% окружения!
\declaretheorem[
style=definition, 
%thmbox=M,
title=Теорема, 
numberwithin=section, 
]{theorem}


\declaretheorem[
style=definition, 
title=Утверждение, 
numberwithin=section, 
]{lemma}



% опция sibling=theorem посередине даёт сквозную нумерацию с теоремами
\declaretheorem[
style=definition, 
title=Определение, 
sibling=theorem, 
]{definition}

\declaretheorem[
style=definition, 
title=Задача, 
% sibling=theorem, 
]{problem}


\declaretheorem[
style=definition,
title=Решение,
numbered=no
]{sol}


%\newenvironment{problem}{}{} % environment just prints the text
% \newenvironment{sol}{}{}
\newcommand{\trru}[1]{#1}
\newcommand{\tren}[1]{#1}



\begin{document}

% задачи можно дёргать:
% https://github.com/bdemeshev/metrics_pro/blob/master/metrics_body.tex
% и ещё (слегка модифицируя)
% https://github.com/bdemeshev/em_pset
% картиночки и доказательства на английском:
% https://github.com/olyagnilova/gauss-markov-pythagoras

\tableofcontents{}



\section{Методы получения оценок}

Методы получения оценок: метод максимального правдоподобия, метод моментов, метод наименьших квадратов.


\section{Свойства оценок}
Свойства оценок: несмещённость, состоятельность, эффективность в классе.

\subsection{Практическое напоминание об условном ожидании и дисперсии}

Вспомним условное ожидание и условную дисперсию в дискретном случае:
\begin{problem}
Совместный закон распределения пары случайных величин $(x, y)$ задан таблицей:

\begin{tabular}{ccc}
\toprule
     & $y = 1$ & $y = 3$ \\
\midrule
$x = 1$ & 0.1 & 0.3 \\
$x = 2$ & 0.1 & 0.1 \\
$x = 4$ & 0.2 & 0.2 \\
\bottomrule
\end{tabular}

\begin{enumerate}
    \item Найдите $\E(y \mid x)$, $\Var(y \mid x)$.
    \item Найдите $\E(y)$, $\E(x)$, $\Cov(x, y)$, $\Var(x)$.
    \item Найдите наилучшее линейное приближение $\BestLin(y \mid x)$.
\end{enumerate}

    \begin{sol}
    \begin{enumerate}
    \item Условное математическое ожидание и дисперсия:
   \[
   \E(y \mid x) = \begin{cases}
   2.5, & x = 1, \\
   2, & x = 2, 4,
   \end{cases}
   \]
   \[
   \Var(y \mid x) = \begin{cases}
   0.75, & x = 1, \\
   1, & x = 2, 4.
   \end{cases}
   \]
    \item Математические ожидания, ковариация и дисперсия:
   \[
   \E(y) = 2.2, \quad \E(x) = 2.4, \quad \Cov(x, y) = -0.28, \quad \Var(x) = 1.84.
   \]
    \item Наилучшее линейное приближение:
   \[
   \BestLin(y \mid x) \approx 2.57 - 0.15 \cdot x.
   \]
   \end{enumerate}
    \end{sol}
\end{problem}

Теперь вспомним, как считать условные характеристики случайных величин при наличии совместной плотности:
\begin{problem}
Пара случайных величин $(x, y)$ имеет функцию плотности
\[
f(x, y) = \begin{cases}
(2x + 4y)/3, \text{ если } x\in [0, 1], y\in [0, 1], \\
0, \text{ иначе.}
\end{cases}
\]

\begin{enumerate}
    \item Найдите $\E(y \mid x)$, $\Var(y \mid x)$.
    \item Найдите $\E(y)$, $\E(x)$, $\Cov(x, y)$, $\Var(x)$.
    \item Найдите наилучшее линейное приближение $\BestLin(y \mid x)$.
\end{enumerate}

    \begin{sol}
        \todo[inline]{Здесь мудрый ассист напишет решение}
    \end{sol}
\end{problem}


Особо обратим внимание на случай двумерного нормального распределения:
\begin{problem}
Пара случайных величин $(x, y)$ имеет совместное нормальное распределение
\[
\begin{pmatrix}
    x \\
    y 
\end{pmatrix} \sim \cN\left(\begin{pmatrix}
    3 \\
    2 \\
\end{pmatrix}, 
\begin{pmatrix}
    10 & -2 \\
    -2 & 20
\end{pmatrix}\right)
\]

\begin{enumerate}
    \item Найдите $\E(y \mid x)$, $\Var(y \mid x)$.
    \item Найдите $\E(y)$, $\E(x)$, $\Cov(x, y)$, $\Var(x)$.
    \item Найдите наилучшее линейное приближение $\BestLin(y \mid x)$.
\end{enumerate}

    \begin{sol}
        \todo[inline]{Здесь храбрый ассист напишет решение}
    \end{sol}
\end{problem}

Обратите внимание: для совместного нормального распределения условное ожидание $\E(y \mid x)$ и наилучшее линейное приближение $\BestLin(y \mid x)$ идеально совпадают. 
Условное ожидание и линейное приближение совпадут и в том случае, если величина $x$ принимает всего два значения.
Убедимся в этом с помощью простой задачи

\begin{problem}
    На первом шаге Илон Маск случайным образом выбирает одно из двух значений случайной величины $x$, $\P(x = 1) = 0.4$, $\P(x = 2) = 0.6$.
    На втором шаге Шивон Зилис выбирает значение $y$ из экспоненциального распределения $x$ с интенсивностью $x$.

\begin{enumerate}
    \item Найдите $\E(y \mid x)$, $\Var(y \mid x)$.
    \item Найдите $\E(y)$, $\E(x)$, $\Cov(x, y)$, $\Var(x)$.
    \item Найдите наилучшее линейное приближение $\BestLin(y \mid x)$.
\end{enumerate}

    \begin{sol}
        \todo[inline]{Здесь неотразимый ассист напишет решение}
    \end{sol}
\end{problem}


Теперь найдём условное ожидание и условную дисперсию для совместного нормального распределения в общем виде. 



\begin{definition}[наилучшее линейное приближение]
\emph{Наилучшее линейное приближение} величины $r$ с помощью величины $s$ — это линейная функция от $s$,
\[
\BestLin(r \mid s) = \beta_1 + \beta_2 s,
\]
где константы $\beta_1$ и $\beta_2$ находятся из решения задачи оптимизации
$\E((r - \BestLin(r, s)^2) \to \min_{\beta_1, \beta_2}$.
При решении задачи окажется
\[
\beta_1 = \E(r) - \beta_2 \E(s), \quad \beta_2 = \frac{\Cov(r, s)}{\Var(s)}
\]
\end{definition}

\begin{definition}[линейно-независимые случайные величины]
Величины $r$ и $s$ называются \emph{линейно-независимыми}, если $\BestLin(r \mid s) = \E(r)$.
\end{definition}

Линейная независимость является симметричным явлением, $\BestLin(r \mid s) = \E(r)$, 
если и только если $\BestLin(s \mid r) = \E(s)$.


\begin{problem}
Выразите константы $\b_1$ и $\b_2$ в формуле для наилучшего линейного приближения 
\[
\BestLin(r \mid s) = \b_1 + \b_2 s,
\]
исходя из характеристик случайных величин $r$ и $s$. 

\begin{sol}
Выпишем целевую функцию в виде суммы
\[
    \E((r - \BestLin(r, s)^2)  = \Var(r - \b_1- \beta_2 s) + (\E(r - \b_1 - \b_2 s))^2
\]
Заметим, что $\b_1$ не влияет на первое слагаемое, так как дисперсия константы равна нулю.
И при этом, выбрав $\b_1 = \E(r - \b_2 s) = \E(r) - \b_2\E(s)$ мы добьёмся того, что второе слагаемое будет равно нулю, своему наименьшему возможному значению. 

Остаётся минимизировать с помощью $\b_2$ первое слагаемое. 
\[
\Var(r - \beta s) = \Var(r) + \b_2^2 \Var(s) - 2 \b_2 \Cov(r, s) \to \min_{\b_2}.
\]
Перед нами квадратичная функция от $\b_2$, следовательно, 
\[
\b_2 = \frac{\Cov(r, s)}{ \Var(s)}.
\]
Обратите внимание, эта формула — родная «теоретическая» сестра «выборочной» формулы для парной регресcии
\[
\hb_2 = \frac{S_{xy}}{S_{xx}}.
\]
Аналогия между оценкой и истинным коэффициентом действует и для первого коэффициента,
\[
\b_1 = \E(r) - \b_2 \E(s), \quad  \hb_1 = \bar y - \hb_2 \bar x.
\]

И, попутно, мы замечаем, что условие $\Cov(r, s) = 0$ равносильно условию  $\BestLin(r \mid s) = \E(r)$ или условию $\BestLin(s \mid r) = \E(s)$.
\end{sol}
\end{problem}



\section{Асимптотические методы}
Центральная предельная теорема. Лемма Слуцкого. Дельта-метод. Построение асимптотических доверительных интервалов.


\section{Святая троица тестов}
Три классических теста: LM, LR, Wald.


Чёрный трек: тесты в матричной форме для вектора параметров?
\subsection{Черный трек}
\subsection{Тройка тестов в матричной форме}
Рассмотрим применение тестов W  (тест Вальда), LR (тест отношения правдоподобия) и LM (тест множителей Лагранжа) для тестирования гипотез о параметрах модели.

Пусть требуется протестировать систему ограничений относительно вектора неизвестных параметров
\[
H_0: \begin{cases}
g_1(\theta) = 0 \\
g_2(\theta) = 0 \\
\ldots \\
g_r(\theta) = 0 \\
\end{cases}
\]
где $g_i(\theta)$ — функция, которая задаёт $i$-е ограничение на вектор параметров $\theta$, $i = 1,\ldots, r$.

Введём следующие обозначения: 

\[
\frac{\partial g}{\partial \theta^T} = \begin{pmatrix}
\partial g_1/\partial \theta^T \\
\partial g_2/\partial \theta^T \\
\vdots \\
\partial g_r/\partial \theta^T \\
\end{pmatrix} = \begin{pmatrix}
\frac{\partial g_1}{\partial \theta_1} & \frac{\partial g_1}{\partial \theta_2} & \ldots & \frac{\partial g_1}{\partial \theta_k}\\
\frac{\partial g_2}{\partial \theta_1} & \frac{\partial g_2}{\partial \theta_2} & \ldots & \frac{\partial g_2}{\partial \theta_k}\\
\ldots & \ldots & \ldots & \ldots \\
\frac{\partial g_r}{\partial \theta_1} & \frac{\partial g_r}{\partial \theta_2} & \ldots & \frac{\partial g_r}{\partial \theta_k}\\
\end{pmatrix}
\]

\[
\frac{\partial g^T}{\partial \theta} = \begin{pmatrix}
\frac{\partial g^T_1}{\partial \theta} & \frac{\partial g^T_2}{\partial \theta} & \ldots & \frac{\partial g^T_r}{\partial \theta} \\
\end{pmatrix} = \begin{pmatrix}
\frac{\partial g_1}{\partial \theta_1} & \frac{\partial g_2}{\partial \theta_1} & \ldots & \frac{\partial g_r}{\partial \theta_1}\\
\frac{\partial g_1}{\partial \theta_2} & \frac{\partial g_2}{\partial \theta_2} & \ldots & \frac{\partial g_r}{\partial \theta_2}\\
\ldots & \ldots & \ldots & \ldots \\
\frac{\partial g_1}{\partial \theta_k} & \frac{\partial g_2}{\partial \theta_k} & \ldots & \frac{\partial g_r}{\partial \theta_k}\\
\end{pmatrix},  \frac{\partial \ell}{\partial \theta}= \begin{pmatrix}
\frac{\partial \ell}{\partial \theta_1} \\
\frac{\partial \ell}{\partial \theta_2} \\
\vdots \\
\frac{\partial \ell}{\partial \theta_k} \\
\end{pmatrix}
\]

\[
I(\theta) = -E \left(\frac{\partial^2 \ell}{\partial \theta \partial \theta^T}\right) = - \E \begin{pmatrix}
\frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_1} & \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_2} & \ldots & \frac{\partial^2 \ell}{\partial \theta_1 \partial \theta_k} \\
\frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_2} & \ldots & \frac{\partial^2 \ell}{\partial \theta_2 \partial \theta_k} \\
\ldots & \ldots & \ldots & \ldots \\
\frac{\partial^2 \ell}{\partial \theta_k \partial \theta_1} & \frac{\partial^2 \ell}{\partial \theta_k \partial \theta_2} & \ldots & \frac{\partial^2 \ell}{\partial \theta_k \partial \theta_k} 
\end{pmatrix}
\]
— информационная матрица Фишера

$\Theta_{UR} := \Theta$ — множество допустимых значений вектора неизвестных параметров без учёта ограничений

$\Theta_{R} := \{ \theta \in \Theta: g(\theta) = 0\}$ — множество допустимых значений вектора неизвестных параметров с учётом ограничений

$\hat{\theta}_{UR} \in \Theta_{UR}$ — точка максимума функции $\ell$ на множестве $\Theta_{UR}$

$\hat{\theta}_{R} \in \Theta_{R}$ — точка максимума функции $\ell$ на множестве $\Theta_{R}$

Тогда для тестирования гипотезы $H_0$ можно воспользоваться одной из следующих ниже статистик:

$LR := -2(\ell(\hat{\theta}_{R}) - \ell(\hat{\theta}_{UR})) \overset{as.}{\sim} \chi^2_r$ — статистика отношения правдоподобия

$W := g^T(\hat{\theta}_{UR}) \cdot \left[ \frac{\partial g}{\partial \theta^T}(\hat{\theta}_{UR}) \cdot I^{-1}(\hat{\theta}_{UR}) \cdot \frac{\partial g^T}{\partial \theta}(\hat{\theta}_{UR}) \right]^{-1} g(\hat{\theta}_{UR}) \overset{as.}{\sim} \chi^2_r$ — статистика Вальда

$LM := \left[ \frac{\partial \ell}{\partial \theta}(\hat{\theta}_{R}) \right]^T \cdot I^{-1}(\hat{\theta}_{R}) \cdot \left[ \frac{\partial \ell}{\partial \theta}(\hat{\theta}_{R}) \right] \overset{as.}{\sim} \chi^2_r$ — статистика множителей Лагранжа


\section{ШБ — МНК}
МНК в скалярной и матричной форме без статистических свойств. Строгая мультиколлинеарность. 

Определим матрицу оператора ортогонального проектирования на подпространство, порожденное векторами $x_j$, $j=1, \dots, k$:
\[
H = X(X^TX)^{-1}X^T.
\]

Матрица--проектор (hat-matrix) $H$ является
\begin{itemize}
    \item симметричной, то есть $H^T = H$
    \[
    H^T = X(X^{T}X)^{-1}X^{T} = H
    \]
    \item идемпотентной, то есть как $H^2 = H$
    \[
    H^2 =  X(X^{T}X)^{-1}(X^{T}X)(X^{T}X)^{-1}X^{T} = X(X^{T}X)^{-1}X^{T} = H
    \]
    \item $\rank H = \trace H = k$
    \[
    \trace H = \trace (X(X^{T}X)^{-1}X^{T}) = \trace ((X^{T}X)(X^{T}X)^{-1}) = \trace I_k = k.
    \]
    Здесь мы использовали свойство следа: $\trace(ABC) = \trace(CAB)$, $A,B,C$ — матрицы.
\end{itemize}

Определим матрицу $M = I-H$. Несложно убедиться, что матрица $M$ так же, как и матрица $H$ симметричная и идемпотентная. При этом $\trace M = \trace(I_n - H) = \trace I_n - \trace H = n - k$.

Из геометрического смысла матриц $H$ и $M$ следует, что
\[
HX = X, \,\,MX = 0.
\]

Выразим вектор остатков в матричном виде:
\[
\hat \u = y - \hat y = y - Hy = (I - H)y = My = M(X\b + u) = Mu.
\]

Пусть 
$s = 
 \begin{pmatrix}
  1 & 1 & \cdots & 1
 \end{pmatrix}^{T}$  — вектор размерности $n \times 1$, состоящий из единиц.
Определим матрицу $\pi = s^{T}(s^{T}s)^{-1}s^{T}$. Матрица $\pi$ — это матрица размерности $n \times n$ вида
\[
\pi = \frac{1}{n}
 \begin{pmatrix}
  1 & 1 & \cdots & 1\\
  1 & 1 & \cdots & 1\\
  \cdots & \cdots & \cdots & \cdots \\
  1 & 1 & \cdots & 1
 \end{pmatrix}.
 \]
 
В качестве домашнего упражнения покажите, что $\pi c = \bar c$, где $c$ — произвольный вектор размерности $n \times 1$.

Используя введенные обозначения, запишем $\TSS$, $\ESS$ и $\RSS$ в матричном виде:
\[
\TSS = (y - \bar{y})^{T}(y - \bar{y}) = (y - \pi y)^{T}(y - \pi y) = (y(I - \pi))^{T}(y(I - \pi)) = (I - \pi)^{T}y^{T}y(I-\pi) = y^{T}(I - \pi)y
\]
\[
\ESS = (\hat{y} - \bar y)^{T}(\hat{y} - \bar y)) = (Hy - \pi y)^{T}(Hy - \pi y) = (y(H - \pi))^{T}(y(H - \pi)) = (H - \pi)^{T}y^{T}y(H - \pi) = y^{T}(H - \pi)y
\]
\[
\RSS = (y - \hat{y})^{T}(y - \hat{y}) = (y - H y)^{T}(y - Hy) = (y(I - H))^{T}(y(I - H)) = (I - H)^{T}y^{T}y(I-H) = y^{T}(I - H)y
\]

Рассказать, что коэффициенты при стандартизации всех переменных называют частными корреляциями. 

Коммент: Здесь первый раз говорим слова «строгая мультиколлинеарность».

Чёрный трэк: нелинейный мнк численно?

Задачи для доски:

МНК и R2 руками на доске

Задачи для колаба:

МНК и R2

Рост R2 c ростом числа регрессоров

Рост \RSS с ростом числа наблюдений


\subsection{Чёрный трэк}

\begin{definition}[LOOCV]
Кросс-валидация с поочередным выкидыванием отдельных наблюдений. 
На английском языке она часто сокращается LOOCV (leave one out cross validation).

Рассмотрим модель $y=X\beta + u$. 

Оценим модель без первого наблюдения. Получим МНК-оценки $\hb^{(-1)}$.
С помощью этих оценок спрогнозируем первое наблюдение, получим прогноз $\hat y_1^{CV}$ и ошибку прогноза $\hat u_1^{CV}$.

Вернём первое наблюдение в выборку и удалим второе наблюдение. Получим МНК-оценки $\hb^{(-2)}$.
С помощью этих оценок спрогнозируем второе наблюдение, получим прогноз $\hat y_2^{CV}$ и ошибку прогноза $\hat u_2^{CV}$.

Поступим так с каждым наблюдением. На выходе получим вектор кросс-валидационных прогнозов $\hat y^{CV}$ и вектор кросс-валидационных ошибок прогнозов $\hat u^{CV}$.    
\end{definition}

\begin{theorem}[связь обычных и кросс-валидационных остатков]
Если модель $y=X\beta + u$ оценивается с помощью МНК и проводится кросс-валидации с поочередным выкидыванием отдельных наблюдений, то:
\[
\hat u_i = (1 - H_{ii}) \cdot \hat u_i^{CV},
\]
где $H$ — матрица-шляпница $H = X(X^TX)^{-1}X^T$, $\hat u$ - остатки регрессии, а $\hat u^{CV}$ — кросс-валидационные ошибки прогнозов.
\end{theorem}

Заметим, что сомножитель $(1 - H_{ii}) \in (0;1)$. 
Другими словами, теорема численно формализует интуитивно ожидаемый результат: кросс-валидационные остатки по знаку совпадают с обычными остатками, а по абсолютной величине — больше, так как соответствующее наблюдение не используется при оценивании коэффициента. 

\begin{proof}
Оценим модель без последнего наблюдения, $\hat y^{-} = X^{-} \hb^{-}$. 

Создадим вектор $y^{*}$, который будет отличаться от $y$ только последним, $n$-м элементом:
вместо настоящего $y_n$ там будет стоять прогноз по модели без последнего наблюдения $\hat y^{-}_n$.

Раз уж мы добавили новую точку лежащую ровно на выборочной регрессии, то при оценки модели
$\hat y^* = X \hat \beta^*$ мы получим в точности старые оценки $\hat \beta^* = \hat \beta^-$. 
Следовательно, и прогнозы эти две модели дают одинаковые, $\hat y_i^* = \hat y_i^-$.

А теперь посмотрим на последний элемент вектора $v = H (y^* - y)$.

С одной стороны, он равен последней строке матрицы $H$ умножить на вектор $(y^* - y)$. 
В векторе $(y^* - y)$ только последний элемент ненулевой, поэтому $v_n = H_{nn} (\hat y^{-}_n - y_n)$.

С другой стороны, мы можем раскрыть скобки, и заметить, что $v = Hy^* - Hy$. 
И окажется, что $v_n = \hat y_n^* - \hat y_n = \hat y_n^- - \hat y_n$.

Отсюда
\[
 \hat y_n^- - \hat y_n = H_{nn} (\hat y_n^- - y_n)
\]

Приводим подобные слагаемые и добавляем слева и справа $y_n$, получаем как раз то, что нужно:
\[
y_n - \hat y_n   = (1 - H_{ii}) (y_n - \hat y_n^- )
\]
\end{proof}


\subsection{Задачи для ДЗ}
\begin{problem}
Пусть 
$s = 
 \begin{pmatrix}
  1 & 1 & \cdots & 1
 \end{pmatrix}^{T}$ — вектор размерности $n \times 1$, состоящий из единиц.
Определим матрицу $\pi = s^{T}(s^{T}s)^{-1}s^{T}$. Матрица $\pi$ — это матрица размерности $n \times n$ вида
\[
\pi = \frac{1}{n}
 \begin{pmatrix}
  1 & 1 & \cdots & 1\\
  1 & 1 & \cdots & 1\\
  \cdots & \cdots & \cdots & \cdots \\
  1 & 1 & \cdots & 1
 \end{pmatrix}.
 \]
 
Покажите, что $\pi c = \bar c$, где $c$ — произвольный вектор размерности $n \times 1$.
\end{problem}
\section{Предпосылки о математическом ожидании и дисперсии}
В этой главе мы познакомимся с понятиями независимости и линейной независимости; расчётом математических ожиданий, ковариаций и дисперсий в матричном виде.

Добавим в метод наименьших квадратов ряд статистических предпосылок на ожидание и дисперсию.

Сформулируем и докажем теорему Гаусса~- Маркова (которая пообещает, что МНК-оценки будут обладать свойствами несмещённости и эффективности).

\subsection{Иерархия зависимостей случайных величин}


Напомним определение наилучшей линейной аппроксимации
\begin{definition}[наилучшая линейная аппроксимация]
Наилучшее линейное приближение величины $r$ с помощью величины $s$ — это линейная функция от $s$,
\[
\BestLin(r \mid s) = \beta_1 + \beta_2 s,
\]
где константы $\beta_1$ и $\beta_2$ находятся из решения задачи оптимизации
$\E((r - \BestLin(r, s)^2) \to \min_{\beta_1, \beta_2}$.
При решении задачи оказывается, что
\[
\beta_1 = \E(r) - \frac{\Cov(r, s)}{\Var(s)} \E(s), \quad \beta_2 = \frac{\Cov(r, s)}{\Var(s)}.
\]
\end{definition}

\begin{definition}[линейная независимость]
Величины $r$ и $s$ называются линейно-независимыми, если $\BestLin(r \mid s) = \E(r)$
\end{definition}
Некоторые авторы считают условие $\Cov(r, s) = 0$ определением линейной независимости.

Можно выделить три степени независимости случайных величин. 
Рассмотрим их на примере пары произвольных величин $r$ и $s$.

\begin{tikzpicture}[node distance=2cm] % Set node distance
    % Define nodes
    \node[rectangle, draw=orange, line width=1.5pt] (indep) {$r$ и $s$ независимы}; 
    \node[rectangle, draw=orange, line width=1.5pt, right=2cm of indep] (indep2) {$\Cov(h_1(r), h_2(s)) = 0$ для любых функций $h_1$ и $h_2$}; 

    \node[rectangle, draw=orange, line width=1.5pt, below of=indep] (noreg) {$\E(r \mid s) = \E(r)$}; 
    \node[rectangle, draw=orange, line width=1.5pt, right=3cm of noreg] (noreg2) {$\Cov(r, h(s)) = 0$ для любой функции $h$}; 

    \node[rectangle, draw=orange, line width=1.5pt, align=center, below of=noreg] (nocov) {$r$ и $s$ линейно-независимы, \\
    $\BestLin(r \mid s) = \E(r)$}; 
    \node[rectangle, draw=orange, line width=1.5pt, right=3cm of nocov] (nocov2) {$\Cov(r, s) = 0$}; 

    % Connect nodes with arrows
    \draw[{Latex[length=5mm, width=4mm]}-{Latex[length=5mm, width=4mm]}, line width=1pt, double distance=3pt] (indep) -- (indep2);
    \draw[{Latex[length=5mm, width=4mm]}-{Latex[length=5mm, width=4mm]}, line width=1pt, double distance=3pt] (noreg) -- (noreg2);
    \draw[{Latex[length=5mm, width=4mm]}-{Latex[length=5mm, width=4mm]}, line width=1pt, double distance=3pt] (nocov) -- (nocov2);

    \draw[-{Latex[length=5mm, width=4mm]}, line width=1pt, double distance=3pt] (indep) -- (noreg);
    \draw[-{Latex[length=5mm, width=4mm]}, line width=1pt, double distance=3pt] (noreg) -- (nocov);
\end{tikzpicture}

Напомним, что определение независимых случайных величин,
\begin{definition}[независимость случайных величин]
Cлучайные величины $r$ и $s$ называются \emph{независимыми}, если 
для любых\footnote{Не совсем любых, требуется измеримость множеств. 
В рамках нашего курса мы не будем обращать внимания на данный нюанс.} числовых множеств $A$ и $B$ независимы события $\{r \in A\}$ и $\{s \in B\}$:
\[
\P(r \in A, s \in B) = \P(r \in A) \cdot \P(s \in B)
\]
\end{definition}

Из независимости величин $r$ и $s$ следует, что информация, известная об $s$, никак не помогает угадывать значение $r$. Поэтому условное математическое ожидание для $r$ равно безусловному. Точно также из независимости $r$ и $s$ следует $\E(s \mid r) = \E(s)$. Обратное утверждение неверно, что показывает контрпример ниже.

\begin{problem}
    Покажем, что из равенства условного и безусловного математических ожиданий не следует независимость случайных величин. Пусть дискретные случайные величины $r$ характеризуют погоду (-1 снег, 1 солнце, 0 дождь), $s$ -- наличие зонта (0 нет или 1 есть) и ниже приведена таблица их совместного распределения.
    
    \begin{tabular}{|l|l|l|l|}
    \hline
      & 1/3 & 1/3 & 1/3 \\ \hline
    r & -1  & 1   & 0   \\ \hline
    s & 0   & 0   & 1   \\ \hline
    \end{tabular}  

    
    
    \begin{sol}
    Уже по формулировке подозреваем, что величины зависимые :).

    Найдём условное ожидание зонта при условии, что мы видим погоду на улице: \( \E(s \mid r) = \begin{cases} 0 \text{ если } r\in \{-1,  1\}, \\
    1, \text{ если } r = 0. \end{cases}\) Получается, что информация о погоде помогает предсказать наличие зонтика, события не являются независимыми.

    Найдём ожидания о погоде за окном, если вы можете наблюдать наличие или отсутствие зонта у человека:  \( \E(r \mid s) = \begin{cases} (-1) \times 1/6 + 1 \times 1/6 = 0, \text{ если } s = 0, \\
    0, \text{ если } s = 1. \end{cases}\).

    Обычное безусловное ожидание погоды на улице: \( \E(r) = (-1) \times 1/3 + 1 \times 1/3 + 0 \times 1/3 = 0\) 

    Получается, что \( \E(r \mid s) = \E(r) = 0, \) но события зависимы.
    \end{sol}
\end{problem}

Вернемся к тому факту, что из равенства условного и безусловного математических ожиданий следует нулевая ковариация. Используя закон повторных математических ожиданий  \( \Cov(r, s) = \E(rs) - \E(r)\E(s) = \E(\E(rs \mid s)) - \E(r)\E(s) = \E(s\E(r \mid s)) - \E(r)\E(s) = \E(r)\E(s) - \E(r)\E(s) = 0.\)


\begin{problem}
    Из нулевой ковариации не следует равенство условного и безусловного математических ожиданий (и тем более не следует независимость). Пусть случайная величина $s$ имеет равномерное распределение на отрезке $[-1; 1]$, а $r = s^2$.

    \begin{sol}

    Напоминаем, что для равномерно распределённой случайной величины \(\E(s) = \frac{-1+1}{2} = 0, pdf(s) = \frac{1}{1 - (-1)} = \frac{1}{2}.\)
    
    \(\E(r \mid s) = \E(s^2 \mid s) = s^2 \neq 0\) в общем случае.

    При этом \(\Cov(r, s) = \E(rs) - \E(r)\E(s) = \E(s^3) - \E(s^2) \times 0 = \E(s^3).\)

    Математическое ожидание сложной функции \( \E(g(x)) = \int^a_b g(x) pdf(x) dx, \text{если } x \in [a, b].\)
    
    Найдём \(\E(s^3) = \int^{1}_{-1} s^3 pdf(s) ds = \int^{1}_{-1} s^3 \frac{1}{2} ds = \frac{1}{8} s^4 | ^1_{-1} = 0.\) Значит, мы получили нулевую ковариацию у зависимых случайных величин.    
    \end{sol}
\end{problem}

\subsubsection*{Вывод}
Существуют независимые случайные величины, но на ???

\subsection{Ожидание и ковариационная матрица }

Пусть $r$ — случайный вектор размерности $n \times 1$, $s$ — случайный вектор размерности $k \times 1$, $A$ и $b$ — неслучайные матрица и вектор соответственно, имеющие подходящие размерности.

Математическим ожиданием случайного вектора $r$ называется вектор
\[
\E(r) = \begin{pmatrix}
	\E(r_1)  \\
	\E(r_2)  \\
        \dots \\
        \E(r_n)
      \end{pmatrix}.
\]

Ковариационная матрица вектора $r$ определяется следующим образом:
\[
\Var(r) = \begin{pmatrix}
	\Cov(r_1,r_1) & \Cov(r_1,r_2) & \dots & \Cov(r_1,r_n) \\
	\Cov(r_2,r_1) & \Cov(r_2,r_2) & \dots & \Cov(r_2,r_n) \\
        \dots & & \dots & \dots\\
        \Cov(r_n,r_1) & \Cov(r_n,r_2) & \dots & \Cov(r_n,r_n) \\
      \end{pmatrix}.
\]

Ковариационная матрица векторов $r$ и $s$ определяется следующим образом:
\[
\Cov(r,s) = \begin{pmatrix}
	\Cov(r_1,s_1) & \Cov(r_1,s_2) & \dots & \Cov(r_1,s_k) \\
	\Cov(r_2,s_1) & \Cov(r_2,s_2) & \dots & \Cov(r_2,s_k) \\
        \dots & \dots & \dots & \dots\\
        \Cov(r_n,s_1) & \Cov(r_n,s_2) & \dots & \Cov(r_n,s_k) \\
      \end{pmatrix}.
\]

Свойства вектора математических ожиданий и ковариационной матрицы:
\begin{enumerate}
    \item $\E(Ar+b) = A\E(r)+b$
    \item $\Cov(r,s) = \E(rs^T)-\E(r)\E(s^T)$
    \item $\Cov(Ar+b,s) = A\Cov(r,s)$
    \item $\Cov(r,As+b) = \Cov(r,s)A^T$
    \item $\Var(r)=\Cov(r,r) = \E(rr^T)-\E(r)\E(r^T)$
    \item $\Var(Ar+b) = A\Var(r)A^T$
    \item $\E(r^T Ar) = \trace(A\Var(r))+\E(r^T) A\E(r)$  
    \item Если вектора $r$ и $s$ имеют одинаковый размер, то
    $\Var(r + s) = \Var(r) + \Var(s) + \Cov(r, s) + \Cov(s, r)$
\end{enumerate}

Условные ожидание и дисперсия определяются аналогично и обладают аналогичными свойствами.
Главное — не забывать ставить вертикальную палочку!

Например, 
\todo[inline]{написать пример}



\subsection{Теорема Гаусса~— Маркова}

\[y = X\b + u\]

Чтобы исследовать свойства полученной точечной оценки $\hb$ нам потребуются предпосылки о математическом ожидании и ковариационной матрице вектора $\u$.

Мы предположим, что случайные ошибки в среднем равны нулю, а именно,
\[
\E(\u \mid X) = 0.
\]


Предпосылку о математическом ожидании можно записать и в скалярном виде,
\[
\E(\u_i \mid X) = 0, \quad \text{ при } \forall i \in \{1, \dots, n\}.
\]

Важно пояснить смысл введённой предпосылки. При оценивании связи между регрессорами $X$ и переменной $y$ мы не предполагаем, что величины $u$ и $X$ независимы. В ошибки модели попадают все те факторы, которые мы забыли включить в регрессию. Эти факторы могут быть взаимосвязаны с тем, что в регрессию всё же попало. Мы делаем более слабое предположение лишь о бесполезности всей собранной в $X$ информации для угадывания $u$ (и следующей из неё линейной независимости между ошибками и регрессорами, в том числе о нулевой ковариации).

\begin{theorem}[Гаусс~— Марков]
Если 
\gaussmarkov
то
\begin{enumerate}[label=(\alph*)]
    \item Оценка $\hb$ является линейной по $y$;
    \item Оценка $\hb$ является условно несмещённой, $\E(\hb \mid X) = \b$ и несмещённой, $\E(\hb) = \b$;
    \item Оценка любого коэффициента $\hb_j$ является наиболее эффективной в классе линейных несмещённых оценок. 
\end{enumerate}
\end{theorem}

Что означает «эффективная в классе линейных несмещённых оценок»?
Это означает, что у любой другой линейной по $y$ несмещённой оценки $\hb^{\alt}_j$ дисперсия не меньше, чем у МНК-оценки.
\[
\Var(\hb_j \mid X) \leq \Var(\hb^{\alt}_j \mid X).
\]



В иностранной литературе для простоты запоминания используется аббревиатура BLUE -- best linear unbiased estimator. То есть при выполнении условий теоремы Гаусса-Маркова мы получаем несмещённые и наилучшие (в терминах эффективности) оценки в классе всех линейных оценок.

Хорошие оценки подобны хорошему подвенечному платью,
\begin{quotation}
Something Olde, Something New, Something Borrowed, Something Blue, A Sixpence in your Shoe.    
\end{quotation}


Вывод теоремы можно усилить, для любой линейной комбинации коэффициентов $w^T \b$ МНК-оценка $w^T \hb$ эффективнее альтернативной оценки $w^T \hb^{\alt}$:
\[
\Var(w^T\hb_j \mid X) \leq \Var(w^T \hb^{\alt}_j \mid X).
\]

\begin{proof}
Линейность оценки по $y$ видна прямо из её формулы, $\hb = (X^TX)^{-1}X^Ty$.

Проверим условную несмещённость, 
\[
\E(\hb \mid X) = \E((X^TX)^{-1}X^Ty \mid X) = (X^TX)^{-1}X^T\E(y \mid X).
\]
Для удобства посчитаем $\E(y \mid X)$ отдельно,
\[
\E(y \mid X) = \E(X\beta + u \mid X) = X\beta + \E(u \mid X) = X\beta.
\]
И теперь завершаем вычисление $\E(\hb \mid X)$:
\[
\E(\hb \mid X) = (X^TX)^{-1}X^T\E(y \mid X) = (X^TX)^{-1}X^TX\beta = \beta.
\]
Мы доказали условную несмещённость оценки, $\E(\hb \mid X) = \beta$.
Безусловная несмещённость следует из свойства условного ожидания,
\[
\E(\hb) = \E(\E(\hb \mid X)) = \E(\beta) = \beta.
\]

Эффективность МНК-оценок — это реинкарнация теоремы Пифагора. 
Мы увидим, что дисперсия МНК-оценки — это квадрат длины катета, 
дисперсия альтернативной несмещённой оценки — квадрат длины гипотенузы.

Для примера рассмотрим оценку первого коэффициента бета, $\hb_1$.
Доказательство не меняется ни капли, если рассмотреть оценку другого коэффициента, скажем, $\hb_7$ или даже оценку произвольной линейной комбинацию коэффициентов бета, например, $\hb_1 + \hb_2 + \hb_3$.

Итак, у нас есть две оценки, $\hb_1$ и $\hb_1^{\alt}$. 
Обе они линейны по $y$, следовательно, $\hb_1 = a^T y$ и $\hb_1^{\alt} = a^T_{\alt} y$.


Замечаем, что $\Var(\hb_1 \mid X) = \sigma^2 a^Ta$, и $\Var(\hb_1^{\alt}) = \sigma^2 a_{\alt}^Ta_{\alt}$. 
То есть дисперсии пропорциональны квадратам длин векторов $a$ и $a^{\alt}$. 
Осталось доказать, что вектор $a$ не длиннее вектора $a^{\alt}$ :)

Для этого мы докажем, что вектор $a^{\alt}$ — это гипотенуза, а вектор $a$ — катет. 
Нам нужно доказать, что вектор $a - a^{\alt}$ перпендикулярен вектору $a$.

Разобъём доказательство перпендикулярности $a$ и $a-a^{\alt}$ на два шага:

Шаг 1. Вектор $a - a^{\alt}$ перпендикулярен любому столбцу матрицы $X$.

Шаг 2. Вектор $a$ является линейной комбинацией столбцов матрицы $X$.


\todo[inline]{здесь простая картинка с теоремой Пифагора!}




Приступаем к шагу 1. Обе оценки несмещённые, поэтому для любых $\beta$ должно выполняться:

\[
\E(\hb_1 \mid X) = \E(\hb_1^{\alt} \mid X)
\]

Переносим всё в левую сторону:

\[
\E((a^T - a^T_{\alt})(X\beta + u) \mid X) = 0
\]

Получаем, что для любых $\beta$ должно быть выполнено условие:

\[
(a - a_{\alt})^T X\beta = 0
\]

Это возможно только, если вектор $(a - a_{\alt})^T X$ равен нулю. 
Следовательно, вектор $(a - a_{\alt})$ перпендикулярен любому столбцу $X$.


Приступаем к шагу 2.

Вспоминаем, что $\hat \beta = (X^T X)^{-1}X^T y$. Следовательно, нужная строка весов $a^T$ — 
это первая строка в матрице $(X^TX)^{-1}X^T$. 
Замечаем, что выражение имеет вид $A \cdot X^T$. 

Вспоминаем из линейной алгебры, что при умножении матриц $AB$ получается матрица $C$, 
на которую можно взглянуть несколькими способами!
Можно считать, что $C$ — это разные линейные комбинации столбцов левой матрицы $A$. 
Можно считать, что $C$ — это разные линейные комбинаций строк правой матрицы $B$.

Применим второй взгляд :) Получаем, что строка $a^T$ — линейная комбинация строк матрицы $X^T$. 
Или, другими словами, столбец $a$ — линейная комбинация столбцов матрицы $X$.
\end{proof}

Классическое доказательство эффективности, которое можно найти во многих учебниках, не замечает связи с теоремой Пифагора и исследует разницу ковариационных матриц. 
Приведём его здесь для демонстрации альтернативной техники!
\begin{proof}
У нас есть две линейных по $y$ оценки: МНК-оценка и оценка-конкурент,
\[
\hb = (X^TX)^{-1}X^T y \text{ и } \hb_{\alt} = A^T_{\alt} y.
\]
Оценки ковариационных матриц этих оценок равны 
\[
\Var(\hb \mid X) = (X^TX)^{-1} \sigma^2 \text{ и } \Var(\hb_{\alt} \mid X) = A^TA \sigma^2.
\]
\end{proof}
Условие несмещённости альтернативной оценки имеет вид 
\[
\E(\hb_{\alt} \mid X) = \E(A^T y \mid X) = A^T X\beta = \beta.
\]
То есть для несмещённости альтернативной оценки должно выполняться условие $A^T X = I$.
Для простоты рассмотрим случай $\sigma^2 = 1$.
Мы докажем, что разница этих матриц $D = A^TA - (X^TX)^{-1}$ является положительно полуопределённой матрицы. 

Вспомним из линейной алгебры определение и свойства положительно полуопределённой матрицы.
\begin{definition}[положительно полуопределённая форма]
Матрица $D$ или квадратичная форма $q(v) = v^T D v$ называется положительно полуопределённой, если $q(v) \geq 0$ для любого вектора $v$.
\end{definition}

\begin{theorem}[свойства положительно полуопределённой матрицы]
    Матрица $D$ является положительно полуопределённой, если и только если её можно записать в виде произведения $D = B^T B$.

    У положительно полуопределённой матрицы $D$ на диагонали находятся неотрицательные числа. 
\end{theorem}

Если $D = A^T A - (X^TX)^{-1}$ — положительно полуопределена, то $d_{ii} \geq 0$ и,
следовательно, $[A^TA]_{ii} \geq [(X^TX)^{-1}]_{ii}$, то есть, дисперсии альтернативных оценок не меньше дисперсий МНК-оценок. 

Перейдём к доказательству положительной полуопределённости $D$:
\begin{proof}
    Возьмём $B = A - X(X^TX)^{-1}$ и найдём $B^TB$:
\[
B^TB = (A - X(X^TX)^{-1})^T (A - X(X^TX)^{-1}) = A^TA - A^T X(X^TX)^{-1}  - (X^TX)^{-1}X^T A + (X^TX)^{-1}X^T X(X^TX)^{-1}
\]    
В силу несмещённости $A^T X = I$ или $X^TA = I$, поэтому
\[
B^TB = A^TA - (X^TX)^{-1}  - (X^TX)^{-1} + (X^TX)^{-1} = A^TA - (X^TX)^{-1}.
\]
Мы видим, что матрица $D = A^TA - (X^TX)^{-1}$ оказалась разложенной в произведение $D = B^TB$ и, следовательно, матрица $D$ положительно полуопределена.
\end{proof}


\subsection{Статистические свойства остатков}
Используя матричное представление для остатков $\hu = My = Mu$, вычислим вектор математических ожиданий и ковариационной матрицы:
\[
\E(\hat u \mid X) = \E(My \mid X) = M\E(y \mid  X) = MX\b = 0, \text{ так как } MX = 0.
\]
Ожидаемое значение остатков равно нулю, также как и ожидаемое значение ошибок, $\E(\hat u \mid X)= \E(u \mid X) = 0$.
\[
\Var(\hat u \mid  X) = \Var(My \mid  X) = M\Var(y \mid  X)M^{T} = M\sigma^2I_nM^{T} = \sigma^2MM^{T} = \sigma^2M.
\]
Вспомним, что у ковариационной матрицы ошибок $\Var(u \mid X) = \sigma^2 I$ на диагонали стоят одинаковые элементы, а вне диагонали стоят нули.
А у ковариационной матрицы остатков $\Var(\hat u \mid X) = \sigma^2 M$ на диагоналях находятся разные элементы и вне диагонали элементы в общем случае не равны нулю.

Другими словами, остатки $\hu_i$ зависимы между собой и имеют разную дисперсию $\Var(\hu_i)$.
Например, при наличии константы в регрессии остатки обязательно удовлетворяют соотношению $\sum \hu_i = 0$.

Посчитаем ковариационную матрицу вектора остатков и вектора прогнозов:
\[
\Cov(\hu, \hy \mid X) = \Cov(M\u, Py \mid X) = \Cov(M\u, P(X\b + u) \mid X) = \Cov(M\u, X\b + Pu \mid X) =
\]
\[
=\Cov(Mu, Pu \mid X) = M \Cov(u,u \mid X) P = M \sigma^2I_n P^{T} = \sigma^2MP = 0, \text{так как } P^{T} = P \text{ и } MP = 0.
\]
Следовательно, вектор остатков и вектор прогнозов линейно независимы. Метод наименьших квадратов даёт наилучший линейный прогноз, то есть даже зная прогнозные значения $\hy$ нет возможности уменьшить остатки модели.

Посчитаем ковариационную матрицу вектора остатков и МНК-оценки вектора параметров $\b$:
\[
\Cov(\hu, \hb \mid X) = \Cov(M\u, \b + (X^{T}X)^{-1}X^{T}u \mid X) = \Cov(Mu,(X^{T}X)^{-1}X^{T}u \mid X) = 
\]
\[
=M\Cov(u, u \mid X) X(X^{T}X)^{-1} = M\Cov(u,u \mid X) X(X^{T}X)^{-1} = \sigma^2 MX(X^{T}X)^{-1}, \text{так как } MX = 0.
\]
Следовательно, вектор остатков и вектор МНК-оценок параметров модели.

\subsection{Оценивание дисперсии}

Метод наименьших квадратов позволяет оценить вектор параметров $\beta$, однако \sout{власти скрывают настоящую дисперсию} никак не оценивает неизвестный параметр $\sigma^2$.
Интуиция говорит, что высокая дисперсия ошибок $u_i$ должна проявляться в высоком разбросе $\hat u_i$, поэтому разумно попробовать построить оценку $\hat\sigma^2$ на базе $\RSS = \sum \hat u_i^2$.

Для построения оценки $\hat\sigma^2$ найдём ожидание $\E(\RSS \mid X)$:
\begin{theorem}[ожидание суммы квадратов остатков]
Если выполнены предпосылки теоремы Гаусса~— Маркова, 
\gaussmarkov
то $\E(\RSS \mid X) = \E(\sum \hu_i^2 \mid X) = (n - k) \sigma^2$.
\end{theorem}
Из этой теоремы следует, что оценка $\hat \sigma^2 = \RSS/ (n - k)$ — несмещённая оценка для неизвестной дисперсии $\sigma^2$.


\begin{proof}
На помощь нам придёт след матрицы! 
След матрицы прекрасен двумя свойствами. 
Во-первых, его можно менять местами с математическим ожиданием, $\E(\trace W)) = \trace \E(W)$. 
Во-вторых, внутри следа можно переставлять местами перемножаемые матрицы, $\trace (AB) = \trace (BA)$. 
Кроме того, на скалярную величину след можно навесить совершенно бесплатно!
Если величина $R$ — не вектор, а скаляр, то $\trace R = R$.

Продолжаем,
\[
\E(\hu^T \hu \mid X) = \E(\trace (\hu^T \hu) \mid X) = \E( \trace(\hu \hu^T)\mid X) = \trace \E(\hu \hu^T \mid X).
\]
Подумаем о середине,
\[
\E(\hu \hu^T \mid X) = \E(Mu (Mu)^T \mid X) = \E(Mu u^T M^T \mid X) = M \E(uu^T \mid X) M^T.
\]
Вспомним, что матрица $M$ — проектор, поэтому $M^T = M$, $M^2 = M$.
У матрицы $uu^T$ на диагонали стоят $u_i^2$, вне диагонали — $u_iu_j$.
Поэтому $\E(uu^T\mid X) = \sigma^2 I$.
Завершаем вычисления,
\[
\E(\hu \hu^T \mid X) =  M \E(uu^T \mid X) M^T= M \cdot \sigma^2 I \cdot M^T = \sigma^2 M^2 = \sigma^2 M
\]
След проектора равен размерности пространства, на которое он проецирует, поэтому $\trace M = n - k$ и
\[
\E(RSS \mid X) = \trace(\sigma^2 M) = (n - k) \sigma^2
\]
И мы легко строим несмещённую оценку, $\hs^2 = \RSS / (n - k)$,
\[
\E( \hs^2 \bar X) = \E \left( \frac{\RSS}{n - k} \mid X \right) = \frac{(n - k) \sigma^2}{ n - k} = \sigma^2
\]

\end{proof}

\subsubsection*{Выборочная дисперсия при случайной выборке}

Заметим, что данная теорема обобщает старый факт про выборочную дисперсию!
Вспомним, что для выборки из независимых $y_i$ c ожиданием $\E(y_i) = \mu$ и дисперсией $\Var(y_i) = \sigma^2$ несмещённая оценка дисперсии имеет вид 
\[
\hat\sigma^2 = \frac{\sum(y_i - \bar y)^2}{n - 1}.
\]
В данном случае величины $y_i$ можно представить в виде $y_i = \mu + u_i$.
Тогда предпосылки теоремы Гаусса~— Маркова выполнены, матрица регрессоров $X$  — это просто единственный столбец-регрессор из единиц, $k = 1$, $\beta = \mu$. 
В этом случае $\hb = \bar y$, все прогнозы равны $\hu_i = \bar y$ и $\RSS = \sum (y_i - \hy_i)^2 = \sum (y_i - \bar y)^2$.
И мы видим, что новая оценка совпадает в этом случае со старой:
\[
\hat\sigma^2 = \frac{\RSS}{n- k} = \frac{\sum (y_i - \hy_i)^2}{ n - 1} = \frac{\sum (y_i - \bar y)^2}{n - 1}
\]

\subsubsection*{Оценка дисперсии оценок коэффициентов}

Для построения доверительных интервалов для коэффициентов $\beta_j$ нам понадобятся оценки дисперсий $\Var(\hb_j \mid X)$.
К счастью, у нас есть несмещённая оценка $\hs^2$ для $\sigma^2$.
Из неё мы легко построим оценку и для неизвестной ковариационной матрицы $\Var(\hb \mid X) = \sigma^2 (X^T X)^{-1}$.
А именно, мы просто подставим оценку дисперсии вместо неизвестной дисперсии:
\[
\hVar(\hb \mid X) = \hs^2 (X^T X)^{-1} = \frac{\RSS}{n - k} (X^TX)^{-1}.
\]

Уточним, что эту оценку мы вывели из предпосылок теоремы Гаусса~— Маркова. 
Если использовать другие предпосылки, то ковариационная матрица $\Var(\hb \mid X)$ перестанет быть равной $\sigma^2 (X^TX)^{-1}$ и нам потребуется другой способ оценивания. 


\subsection{Неправильная спецификация модели}

Одной из предпосылок теоремы Гаусса~— Маркова является правильный выбор спецификации, при котором мы регрессируем $y$ в точности на набор истинных регрессоров. В реальности такое условие вряд ли выполнимо, так как не до всех регрессоров мы способны догадаться. А если догадаемся, то не все сможем измерить или собрать. Можно ли допустить неполную спецификацию модели, но получить BLUE-оценки (несмещённые и эффективные в классе линейных) для собранных регрессоров?

Рассмотрим для начала случай, когда при оценивании модели мы пропускаем часть важных регрессоров. Истинная модель имеет вид
\[
y = W\b + V\gamma + u,
\]
где $W$ — матрица регрессоров размерности $n \times k_1$, $V$ — матрица регрессоров размерности $n \times k_2$. Обозначим через $X = [W \quad V]$ $n \times k$ матрицу всех регрессоров, где $k = k_1 + k_2$.

Вместо истинной модели оценивается следующая модель:
\[
y = W\b + \nu,
\]
где $\nu$ — вектор случайных ошибок в оцениваемой модели.

\begin{lemma}
тут должно быть утверждение про смещённость 
\end{lemma}

\begin{proof}
Пусть $X$ — истинный набор регрессоров, а $W$ — собранный датасет. 
При этом $X = [W \quad V]$. Тогда новая МНК-оценка получается из изменившейся предпосылки о правильности спецификации $\tb = (W^T W)^{-1}W^T y$. Мы бы всё равно хотели получать несмещённую оценку.

\[ \E(\tb \mid W) =  \]

\end{proof}

\begin{lemma}
\label{thm:delta_var_1}
Пусть $\Var(\hb \mid W, V)$ — ковариационная матрица вектора оценок $\beta$, полученного по полному набору регрессоров $X = [W \quad V]$, а $\Var(\tb \mid W)$ — ковариационная матрица вектора оценок $\beta$, полученного по регрессорам  из матрицы $W$. Тогда матрица $\Var(\hb \mid W, V) - \Var(\tb \mid W)$ является положительно полуопределённой матрицей.
\end{lemma}

Утверждение \ref{thm:delta_var_1} означает, что на диагонали матрицы $\Var(\hb \mid W, V) - \Var(\tb \mid W)$ стоят неотрицательные значения. 
В свою очередь, диагональный элемент с индексами ${jj}$ представляет собой разницу дисперсий оценок коэффициента $\b_j$, полученных по полному и по сокращенному набору переменных. 
Это означает, что $\Var(\hb_j \mid W, V) - \Var(\tb_j \mid V) \geq 0$, то есть оценка $\tb_j$ имеет меньшую условную дисперсию. 
Из-за меньшей условной дисперсии оценка $\tb_j$ может получиться более эффективной по сравнению с оценкой $\hb_j$.

\begin{lemma}
    Оценка дисперсии случайной ошибки $\tilde\sigma^2 = \frac{\RSS}{n-k_1}$, полученная по модели с пропущенными переменными, является смещённой,
    \[
    \E(\tilde\sigma^2 \mid W)  \neq \sigma^2
    \]
\end{lemma}

\begin{proof}
Вспомним матричное представление $\RSS$:
\[
\RSS = y^{T}My, \text{ где } M = I_n - W(W^{T}W)^{-1}W^{T}.
\]
Рассчитаем математическое ожидание $\RSS$, учитывая, что истинной моделью является модель по набору регрессоров $X = [W \quad V]$:

\[
\E(\RSS \mid W,V) = \E(y^{T}My \mid W,V) = \E((W\b + V\gamma + u)^{T}M(W\b + V\gamma + u) \mid W,V) = 
\]
\[
=\E(u^{T}Mu + 2\gamma^{T}V^{T}Mu + \gamma^{T}V^{T}MV\gamma \mid W,V) = \sigma^2(n-k_1) + \gamma^{T}V^{T}MV\gamma.
\]

Выше мы использовали следующие результаты:
\begin{itemize}
    \item $MW = 0$;
    \item $\E(2\gamma^{T}V^{T}Mu \mid W,V) = 2\gamma^{T}V^{T}M\E(u \mid W,V) = 0$;
    \item  $\E (u^{T}Mu \mid W,V) = \sigma^2(n-k_1)$.
\end{itemize}

Таким образом, получаем, что 
\[
\E(\tilde\sigma^2 \mid W,V) = \E\left( \frac{RSS}{n-k_1}  \mid W,V\right) = \frac{1}{n-k_1} ( \sigma^2(n-k_1) + \gamma^{T}V^{T}MV\gamma ) = \sigma^2 + \frac{1}{n - k_1} \gamma^{T}V^{T}MV\gamma.
\]
Оценка дисперсии $\tilde\sigma^2$ будет несмещённой только, если $\gamma = 0$. 
Равенство $\gamma = 0$ означает, что пропущенных переменных нет и $X = W$.
Заметим также, что $V^{T}MV = (MV)^{T}MV$, что означает, что матрица $V^{T}MV$ является положительно полуопределённой. 
Следовательно, смещение оценки $\tilde\sigma^2$ в общем случае положительное.
\end{proof}
\begin{comment}
Что будет с эффективностью оценок, если пропущена часть важных регрессоров?
Ковариационная матрица оценок коэффициентов $\tilde{\b}$, полученная по сокращенному набору регрессоров $W$, имеет вид:
\[
\Var(\hb) = \sigma^2(X^{T}X)^{-1}.
\]

Вычислим ковариационную матрицу оценок коэффициентов $[\beta \,\,\,\, \gamma]^{T}$, полученную по полному набору регрессоров:
\[
\Var([\beta \,\,\,\, \gamma]^{T}) = \sigma^2([W \, V]^{T}[W \, V]).
\]

Применяя формулу блочного обращения матриц, получаем
\[
\Var(\tb) = \sigma^2 (W^{T}W - W^{T}V(V^{T}V)^{-1}V^{T}W)^{-1}.
\]

Рассмотрим разность двух ковариационных матриц:
\[
\Var(\hb) - \Var(\tb) = 
\]
\end{comment}

Далее проанализируем, что происходит со свойствами несмещённости и эффективности МНК-оценок в случае включения в модель лишних регрессоров.

Теперь истинной моделью является
\[
y = X\b + u.
\]

Вместо истинной модели оценивается следующая модель:
\[
y = X\b + R\gamma + \nu,
\]
где $R$ — матрица лишних регрессоров.

\begin{lemma}
При включении лишних регрессоров МНК-оценка $\tb$, полученная в модели с набором регрессоров 
$\begin{pmatrix} X & R\end{pmatrix}$, 
остаётся несмещенной, то есть $\E(\tb \mid X, R) = \b$.
\end{lemma}

\begin{lemma}
\label{thm:delta_var_2}
Пусть $\Var(\hb \mid X$ — ковариационная матрица вектора оценок $\beta$, полученного по истинному набору регрессоров $X$, а $\Var(\tb \mid X,R)$ — ковариационная матрица вектора оценок $\beta$, полученного по регрессорам  из матрицы $$\begin{pmatrix} X & R\end{pmatrix}$$. 
Тогда матрица $\Var(\tb \mid X, R) - \Var(\hb \mid X)$ является положительно полуопределённой матрицей.
\end{lemma}

Утверждение \ref{thm:delta_var_2} означает, что на диагонали матрицы $\Var(\tb \mid X, R) - \Var(\hb \mid X)$ стоят неотрицательные значения. 
В свою очередь, диагональный элемент с индексами ${jj}$ представляет собой разницу дисперсий оценок коэффициента $\b_j$, полученных по расширенному и по истинному наборам переменных. 
Это означает, что $\Var(\tb_j \mid X, R) - \Var(\hb_j \mid X) \geq 0$, то есть оценка $\tb_j$ имеет большую условную дисперсию. 
Из-за большей условной дисперсии оценка $\tb_j$ может получиться менее эффективной по сравнению с оценкой $\hb_j$.

\subsection{Задачи для семинара:}
\begin{problem}
Исследовательница Мишель собрала данные по 20 студентам. 
Переменная $y_i$ — количество решённых задач по эконометрике $i$-м студентом, 
а $x_i$ — количество просмотренных серий любимого сериала за прошедший год. 
Оказалось, что $\sum y_i = 10$, $\sum x_i = 0$, $\sum x_i^2 = 40$, $\sum y_i^2 = 50$, $\sum x_i y_i = 60$.

\begin{enumerate}
\item Найдите МНК-оценки коэффициентов парной регрессии.
\item В рамках предположения $\E(u_i \mid X) = 0$ найдите $\E(y_i \mid X)$, $\E(\hb_j \mid X)$, $\E(\hat u_i \mid X)$, $\E(\hat y_i \mid X)$.
\item Предположим дополнительно, что $\Var(u_i \mid X)=\sigma^2$ и $u_i$ при фиксированных $X$ независимы. 
Найдите $\Var(y_i \mid X)$, $\Var(y_i (x_i - \bar x) \mid X)$, $\Var(\sum y_i (x_i - \bar x) \mid X)$, $\Var(\hb_2 \mid X)$.
\end{enumerate}
\begin{sol}
    \todo[inline]{Здесь нужны решения}
  \end{sol}
\end{problem}


\begin{problem}
Рассмотрим модель $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + u_i$, 
 где
\[
X = \begin{pmatrix} 
  1 & 0 & 0 \\ 
  1 & 0 & 0 \\ 
  1 & 1 & 0 \\ 
  1 & 1 & 0 \\ 
  1 & 1 & 1 
\end{pmatrix}, \quad
y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}, \quad
\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}, \quad
u = \begin{pmatrix} u_1 \\ u_2 \\ u_3 \\ u_4 \\ u_5  \end{pmatrix}.
\]
Случайные ошибки $u_i$ независимы и нормально распределены с
$\E(u \mid X) = 0$ и $\Var(u \mid X) = \sigma^2 I$. 

Для удобства расчётов даны матрицы: $X'X$ и $(X'X)^{-1}$ 
\[
X'X = \begin{pmatrix} 
  5 & 3 & 1 \\ 
  3 & 3 & 1\\ 
  1 & 1 & 1 
\end{pmatrix}, \quad
(X' X)^{-1} =  \begin{pmatrix}
  0.5 & -0.5 & 0 \\
  -0.5 & 1 & -0.5 \\
  0 & -0.5 & 1.5 \\
  \end{pmatrix}.
\]


\begin{enumerate}
\item Найдите $\E (\hs^2 \mid X)$, $\hs^2$.
\item Найдите  $\Var (u_1)$, $\Var (\beta_1)$, $\Var (\hb_1 \mid X)$, $\hVar(\hb_1 \mid X)$, $\E (\hb_1^2 \mid X) - \beta_1^2$;
\item Найдите  $\Cov (\hb_2, \hb_3 \mid X)$, $\hCov(\hb_2, \hb_3 \mid X)$, $\Var (\hb_2 - \hb_3 \mid X)$, $\hVar(\hb_2 - \hb_3 \mid X)$;
\item Найдите  $\Var (\beta_2 - \beta_3)$, $\Corr (\hb_2, \hb_3 \mid X)$, $\hCorr(\hb_2, \hb_3 \mid X)$;
\end{enumerate}


\begin{sol}
\todo[inline]{replace 4 by $\sigma^2$}
\todo[inline]{check order of questions}
\begin{enumerate}
\item $\Var(u_1)=\Var(u)_{(1,1)}=4\cdot I_{(1,1)}=4$
\item $\Var(\beta_1)=0$, так как $\beta_1$ — детерминированная величина.
\item $\Var(\hb_1)=\sigma^2(X'X)^{-1}_{(1,1)}=0.5\sigma^2=0.5\cdot 4=2$
\item $\hVar(\hb_1)=\hat\sigma^2(X'X)^{-1}_{(1,1)}=0.5\hat\sigma^2_{(1,1)}=0.5\frac{\RSS}{5-3}=0.25\RSS=0.25y'(I-X(X'X)^{-1}X')y=0.25\cdot 1=0.25$

$\hat\sigma^2=\frac{\RSS}{n-k}=\frac12$.

\item Так как оценки МНК являются несмещёнными, то $\E(\hb)=\beta$, значит:
\[
\E(\hb_1)-\beta_1^2=\E(\hb_1)-(\E(\hb_1))^2=\hVar(\hb_1)=0.25
\]

\item $\Cov(\hb_2,\hb_3)=\sigma^2(X'X)^{-1}_{(2,3)}=4\cdot\left(-\frac12\right)=-2$
\item $\hCov(\hb_2,\hb_3)=\hVar(\hb)_{(2,3)}=\hat\sigma^2(X'X)^{-1}_{(2,3)}=\frac{1}{2}\cdot\left(-\frac12\right)=-\frac14$

\item $\Var(\hb_2-\hb_3)=\Var(\hb_2)+\Var(\hb_3)+2\Cov(\hb_2,\hb_3)=\sigma^2((X'X)^{-1}_{(2,2)}+(X'X)^{-1}_{(3,3)}+2(X'X)^{-1}_{(2,3)}=4(1+1.5+2\cdot(-0.5))=6$

\item $\hVar(\hb_2-\hb_3)=\hVar(\hb_2)+\hVar(\hb_3)+2\hCov(\hb_2,\hb_3)=\hat\sigma^2((X'X)^{-1}_{(2,2)}+(X'X)^{-1}_{(3,3)}+2(X'X)^{-1}_{(2,3)}=\frac{1}{2}\cdot1.5=0.75$

\item $\Var(\beta_2-\beta_3)=0$

\item $\Corr(\hb_2,\hb_3)=\frac{\Cov(\hb_2,\hb_3)}{\sqrt{\Var(\hb_2)\Var(\hb_3)}}=\frac{-2}{\sqrt{4\cdot6}}=-\frac{\sqrt6}{6}$

\item $\hCorr(\beta_2,\beta_3)=\frac{\hCov(\hb_2,\hb_3)}{\sqrt{\hVar(\hb_2)\hVar(\hb_3)}}=\frac{-\frac14}{\sqrt{\frac12\cdot\frac34}}=-\frac{\sqrt6}{6}$

\item $(n-k)\frac{\hat\sigma^2}{\sigma^2}\sim\chi^2_{n-k}$.
\[
\E\left((n-k)\frac{\hat\sigma^2}{\sigma^2}\right)=n-k
\]
\[
\E\left(\frac{\hat\sigma^2}{2}\right)=1
\]
\[
\E(\hat\sigma^2)=2
\]

\item $\hat\sigma^2=\frac{\RSS}{n-k}=\frac{1}{2}$

\end{enumerate}

\end{sol}
\end{problem}





\begin{problem}
Рассмотрим классическую линейную модель $y=X\beta + u$ с предпосылками Гаусса~— Маркова: $\E(u \mid X) = 0$ и $\Var(u \mid X) = \sigma^2 I$.
Для всех случайных векторов ($y$, $\hy$, $\hb$, $u$, $\hat u$, $\bar y$) найдите все возможные ожидания и ковариационные матрицы
$\E(\cdot)$, $\Var(\cdot)$, $\Cov(\cdot, \cdot)$.
\begin{sol}
    \todo[inline]{Здесь нужны решения}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим модель $y_i = \beta x_i + u_i$ с двумя наблюдениями, $x_1 = 1$, $x_2 = 2$.
Величины $u_1$ и $u_2$ независимы и равновероятно равны $+1$ или $-1$.


\begin{enumerate}
  \item Найдите оценку $\hb_{\ols}$ для $\beta$ с помощью метода наименьших квадратов. 
  \item Чему равна дисперсия $\Var(\hb_{\ols} \mid x)$ и ожидание $\E(\hb_{\ols} \mid x)$?
  \item Постройте несмещённую оценку $\hb_{\best}$ с наименьшей дисперсией.
  \item Чему равна дисперсия $\Var(\hb_{\best}\mid x)$?
  \item А как же теорема Гаусса — Маркова? Почему в данном примере удаётся построить оценку с дисперсией меньше, чем у оценки методом наименьших квадратов?

\end{enumerate}
  \begin{sol}
    \begin{enumerate}
      \item $\hb_{\ols} = (y_1 + 2y_2)/5$;
      \item $\Var(\hb_{\ols}\mid x) = 1/5$;
      \item Заметим, что по величине $2y_1 - y_2$ можно однозначно восстановить величины ошибок $u_1$ и $u_2$.
      Например, если $2y_1 -y_2 = 3$, то $u_1 = 1$, $u_2 = -1$.
      \[
        \hb_{\best} = \begin{cases}
          y_1 + 1, \text{ если } 2y_1 - y_2 < 0,\\
          y_1 - 1, \text{ если } 2y_1 - y_2 >0.
        \end{cases}
      \]
      \item Шок контент, $\Var(\hb_{\best}\mid x) = 0$.
      \item Построенная оценка $\hb_{\best}$ является нелинейной по $y$, 
      а теорема Гаусса — Маркова гарантирует только, что метод наименьших квадратов 
      порождает несмещённую оценку с наименьшей дисперсией среди линейных по $y$ оценок. 
    \end{enumerate}
        
  \end{sol}
\end{problem}

\begin{problem}
Предположим, что все предпосылки теоремы Гаусса~— Маркова выполнены. 
Вычислите математические ожидания для $\TSS$, $\ESS$ и $\RSS$, используя их матричные представления.
\begin{sol}
    \todo[inline]{Здесь нужны решения}
\end{sol}
\end{problem}

\begin{problem}(Hansen 4.14)

Задана модель $y = X \beta + u$, для которой выполняются предпосылки теоремы Гаусса~— Маркова. 
Вас интересует величина $\theta = \beta^2$. 
Получены МНК-оценки коэффициентов: $\hb$, $V_{\hb} = \Var [\hb \mid X]$. 
Кажется, неплохой идеей будет оценить $\theta$ как $\hat{\theta} = \hb^2$.

\begin{enumerate}
    \item Найдите $\E [\hat{\theta} \mid X]$. Является ли $\hat{\theta}$ смещённой?
    \item Предложите способ коррекции смещения для получения несмещённой оценки  $\hat{\theta}^*$, используя результаты предыдущего пункта.
\end{enumerate}

\begin{sol}
    \todo[inline]{Здесь нужны решения}
\end{sol}

\end{problem}

\begin{problem}
Рассмотрим модель регрессии $y_i = \beta_1 + \beta_2x_{i2} + \dots +\beta_kx_{ik} + \u_i$. 
Все предпосылки теоремы Гаусса~— Маркова выполнены. 
Дополнительно предположим, что $\u_i \sim \cN(0,\sigma^2), i=1, \dots, n$. 
Дополнительно известно, что на самом деле $\beta_2 = \dots = \beta_k = 0$.
\begin{enumerate}
\item Найдите $\E(R^2)$.\\

\textbf{Решение:}\\
Модель без ограничений: 
\[
y_i = \beta_1 + \beta_2x_{i1} + \dots +\beta_kx_{ik} + \varepsilon_i.
\]

Модель с ограничениями (истинная модель!):

\[
y_i = \beta_1 + \varepsilon_i.
\]

Тогда F-статистика имеет следующий вид:
\[
F = \frac{R^2(k-1)}{(1-R^2)/(n-k)} \sim F(k-1,n-k).
\]

Выразим $R^2$:
\[
R^2(n-k) = F(1-R^2)(n-k)
\]

\textbf{Факт дня №1:} Если $X \sim F(k_1,k_2)$, то $Y = \frac{\frac{k_1}{k_2}X}{1+\frac{k_1}{k_2}X} \sim \dBeta \left(\frac{k_1}{2}, \frac{k_2}{2}\right)$.

Используя факт дня №1, получаем:

\[
R^2 = \frac{(k-1)F}{(n-k) + (k-1)F} = \frac{\frac{k-1}{n-k}F}{1+\frac{k-1}{n-k}F} \sim \dBeta\left(\frac{k-1}{2},\frac{n-k}{2}\right).
\]

Тогда чтобы посчитать математическое ожидание $R^2$, надо вспомнить, чему равно математическое ожидание для $Beta\left(\frac{k-1}{2},\frac{n-k}{2}\right)$:

\[
E(R^2) = \frac{\frac{k-1}{2}}{\frac{k-1}{2}+\frac{n-k}{2}} = \frac{k-1}{n-1}.
\]

Что нам даёт полученный результат? Математическое ожидание коэффициента детерминации линейно по $k$. То есть даже при включении в модель лишних факторов $R^2$ все равно продолжает линейно расти!
\item Найдите $\E(R^2_{\adj})$.\\
\textbf{Решение:}\\
Скорректированный коэффициент детерминации имеет вид:
\[
R^2_{\adj} = 1 - \left(1 - R^2\right)\frac{n-1}{n-k}.
\]
Рассчитаем математическое ожидание:
\[
E(R^2_{\adj}) = E\left(1 - \left(1 - R^2\right)\frac{n-1}{n-k}\right) = 1-\frac{n-1}{n-k}+\frac{n-1}{n-k}E(R^2) = 
\]
\[
= 1-\frac{n-1}{n-k}+\frac{n-1}{n-k}\frac{k-1}{n-1} = 0.
\]
Скорректированный $R^2$ помог решить проблему линейного роста по $k$!
\end{enumerate}
\end{problem}

\begin{problem}
У овечки Долли был набор данных из $n$ наблюдений для которого были выполнены предпосылки теоремы Гаусса~— Маркова. 
Овечка Долли клонировала каждое наблюдение по одному разу и дописала каждое наблюдение-клон сразу после исходного наблюдения. 

\begin{enumerate}
  \item Как выглядит ковариационная матрица ошибок для нового набора данных?
  \item Как изменится ответ на (а), если Долли клонирует только последнее наблюдение $n$ раз?
\end{enumerate}
\begin{sol}
    \begin{enumerate}
        \item Ковариационная матрица будет содержать блоки $B$ на диагонали
\[
\Var(u) = \begin{pmatrix}
            B & 0 & 0 & \dots \\
            0 & B & 0 & \dots \\
            0 & 0 & B & \dots \\
            \dots
        \end{pmatrix},
\]
где каждый блок равен $B = \begin{pmatrix}
    \sigma^2 & \sigma^2 \\
    \sigma^2 & \sigma^2 \\    
\end{pmatrix}$.
        \item Ковариационная матрица будет состоять из четырех блоков: два блока нулевые, левые верхний блок пропорционален единичной матрицы, 
        а все элементы правого нижнего блока равны $\sigma^2$:
\[
\Var(u) = \begin{pmatrix}
            \sigma^2 \cdot I & 0 \\
            0 & S \\
        \end{pmatrix},
\]
где $I$ — единичная матрица, а все $S_{ij} = \sigma^2$.
    \end{enumerate}
\end{sol}
\end{problem}



\subsection{Компьютерные задачи для семинара:}

Генерация R2 для вывода распределения

Генерация смещения

Генерация лишних регрессоров

Реальный пример с лишним регрессорами (тип знаки зодиака и ретроградный)

Какая-то длинная задача, которую из темы в тему и в ней находить потом нарушения предпосылок?



\url{https://colab.research.google.com/drive/1wFrLyGcVVETx96jS93I4z8asgAQwqIdw?usp=sharing}


\subsection{Домашнее задание:}


\subsection{Чёрный трэк:}

\textbf{Умножение блочных матриц.} Если размеры блоков допускают операцию умножения, то:

\[
\left[
\begin{array}{c|c}
A & B \\
\hline
C & D
\end{array}
\right]
\cdot
\left[
\begin{array}{c|c}
E & F \\
\hline
G & H
\end{array}
\right]
=
\left[
\begin{array}{c|c}
AE + BG &  AF+BH\\
\hline
CE+DG & CF+DH
\end{array}
\right].
\]

\bigskip

\textbf{Формула Фробениуса (блочное обращение).}
\[
\left[
\begin{array}{c|c}
A & B \\
\hline
C & D
\end{array}
\right]^{-1}=
\left[
\begin{array}{c|c}
A^{-1}+A^{-1}BH^{-1}CA^{-1} & -A^{-1}BH^{-1} \\
\hline
-H^{-1}CA^{-1} & H^{-1}
\end{array}
\right],
\]

где $A$ — невырожденная квадратная матрица размерности $n \times n$, $D$ — квадратная матрица размерности $k \times k$, $H = D - CA^{-1}B$.

\begin{problem}
Пусть истинной является модель $y = X_1\b_1+ X_2\b_2+u$, где $X_1$, $X_2$  — матрицы признаков размерностей $n \times k_1$ и $n \times k_2$ соответственно. Вместо истинной модели вы оцениваете модель вида $y = X_1\b_1+v$, где $v$ — вектор случайной ошибки, удовлетворяющий предпосылкам теоремы Гаусса~— Маркова.
\begin{enumerate}
    \item Будет ли МНК-оценка вектора параметров $\b_1$ несмещённой?
    \item Будет ли несмещённой МНК-оценка дисперсии случайной ошибки?
    \item Рассчитайте $\Var(\hb_1)$. Не противоречит ли полученной результат теореме Гаусса~— Маркова?
\end{enumerate}
\end{problem}

\begin{problem}
Пусть истинной является модель $y = X_1\b_1+u$, где $X_1$ — матрица признаков размерности $n \times k_1$. Вместо истинной модели вы оцениваете модель вида $y = X_1\b_1+ X_2\b_2+v$, где $X_2$  — матрица признаков размерности  $n \times k_2$, $v$ — вектор случайной ошибки, удовлетворяющий предпосылкам теоремы Гаусса~— Маркова.
\begin{enumerate}
    \item Будет ли МНК-оценка вектора параметров $\b_1$ несмещённой?
    \item Будет ли несмещённой МНК-оценка дисперсии случайной ошибки?
    \item Рассчитайте $\Var(\hb_1)$. Не противоречит ли полученной результат теореме Гаусса~— Маркова?
\end{enumerate}
\end{problem}

\section{Доверительные интервалы для коэффициентов}
Построение доверительных интервалов для МНК оценок. Проверка гипотез. Асимптотика без нормальности ошибок. Нормальность ошибок.


\subsection{Случай многомерного нормального распределения}
\todo[inline]{сопроводить оценкой правдоподобия и показать, что она совпадает с МНК}


Напомним несколько фактов про многомерное нормальное распределение.


Начнём с классического определения:
\begin{definition}[многомерное нормальное распределение]
Вектор $v$ имеет \emph{многомерное невырожденное нормальное распределение}, $v \sim \cN(
\mu, C)$, если его совместная функция плотности равна 
\[
f(v) = (2\pi)^{-n/2} \det(C)^{-1/2} \exp\left(-\frac{1}{2}(v - \mu)^T C^{-1}(v-\mu)\right),
\]    
где $n$ — размерность вектора $v$.
\end{definition}

Заметим, что совместный закон распределения нормального вектора $v$ полностью определён его ожиданием $\E(v)$ и его ковариационной матрицей $\Var(v)$.
Никакие другие параметры в совместную функцию плотности не входят. 

Для многомерного нормального распределения нет разницы между независимостью и некоррелированностью:
\begin{theorem}[некоррелированность и незавимость для нормального вектора]
Если нормальный вектор $v$ состоит из двух подвекторов, $v = (x, y)$, то $\Cov(x, y) = 0$ если и только если подвекторы $x$ и $y$ независимы.     
\end{theorem}
\begin{proof}
    Докажем в одну сторону. 
    Если подвекторы $x$ и $y$ независимы, то $\Cov(x, y) = 0$.
    А теперь изящно докажем в обратную сторону. 
    Если $\Cov(x, y) = 0$, то вся ковариационная матрица $\Var(v)$ \emph{ровно такая же} как и в случае независимых $x$ и $y$. 
    Остаётся лишь вспомнить, что $\E(v)$ и $\Var(v)$ полностью определяют закон распределения нормального вектора $v$, а значит компоненты обязаны быть независимы. 
\end{proof}

Также для многомерного нормального распределения нет разницы между условным ожиданием $\E(y \mid x)$ и наилучшим линейным приближением $\BestLin(y \mid x)$, 
другими словами функция $\E(y \mid x)$ линейна по $x$.

\begin{problem}
Рассмотрим совместное нормальное распределение 
\[
\begin{pmatrix}
    x \\
    y 
\end{pmatrix} \sim \cN\left(
\begin{pmatrix}
    \mu_x \\
    \mu_y \\
\end{pmatrix}, 
\begin{pmatrix}
    C_{xx} & C_{xy} \\
    C_{yx} & C_{yy} \\
\end{pmatrix}\right)
\]
\begin{enumerate}
    \item Найдите наилучшее линейное приближение $\BestLin(y \mid x)$.
    \item Найдите условное ожидание $\E(y \mid x)$.    
    \item Найдиту условную дисперсию $\Var(y \mid x)$.
\end{enumerate}

\begin{sol}
    \todo[inline]{Здесь рассказать про определение bestlin в векторном случае?}
    \begin{enumerate}
    \item Пусть $\BestLin(y \mid x)  = a + B x$.
    Мы хотим, чтобы ошибка линейной аппроксимации $r=y - \BestLin(y \mid x)$ была некоррелирована с $x$,
    \[
    \Cov(y - \BestLin(y \mid x), x) = 0.
    \]
    Другими словами,
    \[
    \Cov(y, x) = \Cov(\BestLin(y \mid x), x) = 0.
    \]
    Подставим $\BestLin(y \mid x ) =a + Bx$.
    \[
    \Cov(y, x) = \Cov(a + Bx, x) = \Cov(Bx, x) = B\Cov(x, x) = B \Var(x).
    \]
    Отсюда $C_{yx} = B C_{xx}$ и $B = C_{yx}C_{xx}^{-1}$.
    Кроме того, ошибка линейной аппроксимации должна иметь нулевое ожидание, следовательно,
    \[
    \E(y) = \E(\BestLin(y \mid x)) = a  + B \E(x).
    \]
    Получаем уравнение на $a$:
    \[
    \mu_y = a + C_{yx} C_{xx}^{-1} \mu_x
    \]
    И ответ,
    \[
    \begin{cases}
        B = C_{yx}C_{xx}^{-1} \\
        a = \mu_y - C_{yx} C_{xx}^{-1} \mu_x
    \end{cases}
    \]
    \item Для нормально распределённой пары векторов  нулевая ковариация равносильная независимости.
    Следовательно, ошибка аппроксимации $r = y - \BestLin(y \mid x)$ и $x$ независимы.
    Отсюда мы получаем, что для многомерного нормально распределённого вектора $(x, y)$
    \[
    \E(y \mid x) = \BestLin(y \mid x) = a + Bx
    \]
    \item Условная дисперсия — это безусловная дисперсия ошибки прогноза,
    \[
    \Var(y \mid x) = \Var(a + Bx + r \mid x) = \Var(r \mid x) = \Var(r).
    \]
    Осталось вспомнить, что $y = a + Bx + r$, прогноз $a + Bx$ и ошибка $r$ некоррелированы,
    \[
    \Var(y) = \Var(a + Bx) + \Var(r).
    \]
    Значит,
    \[  
    \Var(y \mid x) = \Var(r) = \Var(y) - B\Var(x)B^T = C_{yy} - C_{yx}C_{xx}^{-1} C_{xx} C_{xx}^{-1}C_{xy} = C_{yy} - C_{yx}C_{xx}^{-1}C_{xy}.
    \]
    \end{enumerate}
    Отметим, что для компонент $x$ и $y$ нормального вектора $(x, y)$ условная дисперсия получилась постоянной и не зависящей от $x$. 
    
    Для ненормального распределения условное ожидание $\E(y \mid x)$ и условная дисперсия $\Var(y \mid x)$ вполне могут быть нелинейными.
\end{sol}

\end{problem}





Введём дополнительную предпосылку $(\u|X) \sim \cN(0, \sigma^2 I)$.
Учитывая, что $\hb = (X^T X)^{-1}X^T y = \b + (X^T X)^{-1}X^T u$, получаем
\[
(\hb \mid X) \sim \cN(\beta, \sigma^2(X^T X)^{-1}).
\]

\subsection{Независимость оценок $\b$ и $\hat\sigma^2$}
МНК-оценка вектора коэффициентов $\b$ имеет вид
\[
\hb = (X^TX)^{-1}X^{T}y.
\]
Несмещённая оценка дисперсии случайной ошибки:
\[
\hat\sigma^2 = \frac{\RSS}{n-k} = \frac{\hu^T \hu}{n-k}.
\]
Распишем
\[
\hb = (X^TX)^{-1}X^{T}y = (X^TX)^{-1}X^{T}(X\b+u) = \b + (X^TX)^{-1}X^{T}u = \b + Au,
\]
где $A = (X^TX)^{-1}X^{T}$.

В случае, когда случайный вектор ошибок $u$ является нормальным, можно показать, что оценки $\hb$ и $\hu$ будут независимыми.

При $u \sim \cN(0,\sigma^2I_n)$ случайные векторы $\hb$ и $\hu$ имеют совместное многомерное нормальное распределение. Покажем, что  $\hb$ и $\hu$ являются некоррелированными, из чего следует, что они также будет и независимыми, что справедливо для нормально распределенных векторов:

\[
\Cov(\hb,\hu) = \Cov(\b + Au, Mu) = \Cov(Au,Mu) = AM\Var(u) = \sigma^2AM = 0, \textbf{так как } AM=0.
\]

Так как $\hat\sigma^2$ есть функция от случайного вектора $\hu$, то оценки $\hb$ и $\hat\sigma^2$ также независимы.

\subsection{Проверка гипотез о параметрах}
\[
H_0: \b_j = \b_j^0
\]
\[
H_1: \b_j \neq \b_j^0
\]

\[
t = \frac{\hb_j - \b_j^0}{\se(\hb_j)} \overset{H_0}{\sim} t(n-k)
\]

Проверка гипотезы о незначимости модели в целом
\[
H_0: \b_2 = \dots =\b_k = 0
\]
\[
H_1: \sum_{j=2}^k \b_j^2 > 0
\]
\[
F = \frac{R^2 / (k-1)}{(1 - R^2)/(n-k} \overset{H_0}{\sim} F(k-1,n-k).
\]
\section{Бутстрэп}
Бутстрэп. Классический бутстрэп до регрессии и бутстрэп в регрессии. Метод наименьших модулей.

Чёрный трэк: возможно, разные варианты бутстрэпа в регрессии? BCA-бутстрэп до регрессии?

\section{Выбор функциональной формы}
Дамми-переменные и их интерпретация. Функциональные формы: полиномы, логарифмы, интерпретация коэффициентов. Информационные критерии.

Чёрный трэк: Структурные сдвиги. Тест Чоу. Локально-линейная регрессия (LO\ESS).

\section{Гетероскедастичность}
Гетероскедастичность. Тестирование гетероскедастичности. Робастные оценки. Доступный обобщённый МНК.

Задачи для доски:

Хансен: во сколько раз может быть недооценена дисперсия из-за гетероскедастичности


Коммент: акцент на робастных ошибках, тестирование и обобщённый МНК — кратко.

\section{Мультиколлинеарность и метод главных компонент}
Мультиколлинеарность и метод главных компонент.

Чёрный трэк: несколько взглядов на метод главных компонент? LASSO?


\section{Эндогенность}
Эндогенность. Инструментальные переменные. Ошибка измерения регрессора. Двухшаговый МНК.


\section{Эффекты воздействия}
Оценка эффектов воздействия. ATE. LATE. Четкий (sharp) и нечеткий (fuzzy) разрывный регрессионный дизайн (RDD).

Чёрный трэк: Метод разность разностей (DiD). Динамический метода разность разностей (Event Study).

\section{Задачи}

\section{Логистическая регрессия: точечные оценки}
Логистическая регрессия: Бинарный и упорядоченный логит. Точечные оценки, прогнозы.  Интерпретация предельных эффектов.

Чёрный трэк: Множественные логиты. Неупорядоченные, условные, смешанные логиты.

\section{Логистическая регрессия: доверительные интервал}
Логистическая регрессия: доверительные интервалы и проверка гипотез.

Чёрный трэк: разные хоббиты

\subsection{Смещение, цензурирование и $\blacksquare\blacksquare\blacksquare\blacksquare\blacksquare\blacksquare$}

Представим себе ситуацию, в которой зависимая количественная не всегда наблюдаема. 
Для моделирования этой ситуации мы введём скрытую латентная переменная $y_i^*$, которая линейно зависит от предиктора $x_i$, как обычно,
\[
y_i^* = x_i^T \beta + u_i, \quad y^* = X^T \beta + u
\]
Бинарная переменная $z_i \in \{0, 1\}$ равна $1$ в случае, если мы наблюдаем $y_i^*$.

Возможно несколько случаев:

\begin{center}
    \begin{tabular}{lccc}
    	\toprule
            & наблюдаемость $y^*$ & наблюдаемость $x$ & наблюдаемость $w$ \\
        \midrule
         Цензурирование \\ censored model & зависит от  $y^*$ &всегда & \\ 
         Усечение \\ truncated model & зависит от $y^*$ & если наблюдаем $y^*$  \\
         Выборочное смещение \\ sample selection & зависит от $w$  & всегда & всегда \\
         Переключающиеся режимы \\ switching regimes & всегда, $w$ переключает тип зависимости & всегда & всегда \\   
      \bottomrule
    \end{tabular}
\end{center}

Представим себе, что мы открыли дорогой ресторан. 
К нам заглядывают клиенты. 
Часть клиентов ужасаются от ценника и убегают, $y_i^* < 0$.
Часть клиентов остаются и ужинают у нас, $y_i^* > 0$.
Вместо нуля можно выбрать другой порог, но с нулём чуть-чуть удобнее. 


\subsection{Цензурирование}

Рассмотрим самый распространённый вариант цензурирования: вместо отрицательных значений латентной переменной $y_i^*$ мы видим нули.

Эта модель известна как тобит модель типа I, type I Tobit model. 
\[
\begin{cases}
    y_i^* = x_i^T \beta + u_i, \quad y^* = X^T \beta + u \\
    (u \mid X) \sim \cN(0, \sigma^2 I) \\
    y_i = \max\{y_i^*, 0\} \\
    (x_i, y_i) \text{ наблюдаемы при любых }i \\
\end{cases}
\]

Лог-функция правдоподобия равна
\[
\ell(\beta, \sigma) = \sum_{y_i = 0} {\ln F(-x_i^T \beta / \sigma)} + \sum_{y_i > 0} {\ln f((y_i-x_i^T\beta) / \sigma)} - \sum_{y_i > 0} \ln \sigma
\]


\subsection{Усечение}

\[
\begin{cases}
    y_i^* = x_i^T \beta + u_i, \quad y^* = X^T \beta + u \\
    (u \mid X) \sim \cN(0, \sigma^2 I) \\
    y_i = \max\{y_i^*, 0\} \\
    (x_i, y_i) \text{ наблюдаемы, если } y_i > 0 \\
\end{cases}
\]

Лог-функция правдоподобия равна
\[
\ell(\beta, \sigma) = \sum_{y_i > 0} {\ln f((y_i-x_i^T\beta) / \sigma)} - \sum_{y_i > 0}{\ln F(x_i^T \beta / \sigma)} - \sum_{y_i > 0} \ln \sigma
\]


\subsection{Три осмысленных условных ожидания}

Ожидание латентной переменной показывает, \emph{сколько в среднем планирует потратить гость ресторана на ужин, ещё не видевший цен, полезность от ужина},
\[
m^*(x_i) = \E(y^*_i \mid  x_i) = x_i^T \beta
\]
Предельный эффект для латентной переменной 
\[
\partial \E(y^*_i \mid  x_{ij})/ \partial x_{ij} = \beta_j
\]


Ожидание цензурированной переменной, $y_i = \max\{ y_i^*, 0 \}$, \emph{сколько в среднем потратит человек, заглянувший в ресторан, с учётом того, что часть уйдёт испугавшись ценника}
\[
m(x_i) = \E(y_i \mid  x_i) = x_i^T \beta F(x_i^T \beta /\sigma) + \sigma f(x_i^T \beta / \sigma)
\]
Предельный эффект для цензурированной переменной 
\[
\partial \E(y_i \mid  x_{ij})/ \partial x_{ij} = 
\]


Условное ожидание усечённой переменной, $(y_i \mid y_i^* > 0)$, \emph{средний чек в ресторане}
\[
m^\#(x_i) = \E(y_i \mid x_i, y_i^* > 0) = x_i^T \beta  + \sigma \IMR(x_i^T \beta / \sigma),
\]
где $\IMR(s)$ — обратное отношение Миллса, inverse Mills ratio,
\[
\IMR(s) = \E(v \mid v + s > 0) = f(s) / F(s), \quad v\sim \cN(0;1)
\]
Предельный эффект для ожидания усечённой переменной


\subsubsection*{Выборочное смещение}


\subsubsection*{Переключающиеся режимы}




\listoftheorems[ignoreall,show={theorem,definition}]





\end{document}

